{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/55_all/'\n",
    "val_dir = '../data/56_all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, processor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"annotations.json\" if train else \"annotations.json\")\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        img, target = super().__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor\n",
    "\n",
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CocoDetection(img_folder=train_dir, processor=image_processor)\n",
    "val_dataset = CocoDetection(img_folder=val_dir, processor=image_processor, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     data = {}\n",
    "#     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "#     data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "#     if \"pixel_mask\" in batch[0]:\n",
    "#         data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "#     return data\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_full = [\n",
    "    {\"id\": 0, \"name\": \"biker\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 1, \"name\": \"car\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 2, \"name\": \"pedestrian\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 3, \"name\": \"trafficlight\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 4, \"name\": \"trafficlight-Green\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 5, \"name\": \"trafficlight-GreenLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 6, \"name\": \"trafficlight-Red\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 7, \"name\": \"trafficlight-RedLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 8, \"name\": \"trafficlight-Yellow\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 9, \"name\": \"trafficlight-YellowLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 10, \"name\": \"truck\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 11, \"name\": \"Arret\", \"supercategory\": \"objects\"}\n",
    "]\n",
    "categories = [d['name'] for d in categories_full]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = train_dataset.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(train_dir, image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "label2id = {v['name']: k for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "val_batch_size = 32\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "lr_backbone = 1e-5\n",
    "max_steps = 100000 // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=val_batch_size)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import make_dataclass\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         # we specify the \"no_timm\" variant here to not rely on the timm library\n",
    "         # for the convolutional backbone\n",
    "         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "                                                             revision=\"no_timm\",\n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "         # self.config = make_dataclass(\"config\", ['id2label', 'label2id'])(id2label=id2label, label2id=label2id)\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "\n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# trainer = Trainer(max_steps=max_steps, gradient_clip_val=0.1)\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detr.load_from_checkpoint('./lightning_logs/version_5/checkpoints/epoch=240-step=25305.ckpt', lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = convert_to_xywh(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# initialize evaluator with ground truth (gt)\n",
    "evaluator = CocoEvaluator(coco_gt=val_dataset.coco, iou_types=[\"bbox\"])\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    # turn into a list of dictionaries (one item for each example in the batch)\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "\n",
    "    # provide to metric\n",
    "    # metric expects a list of dictionaries, each item\n",
    "    # containing image_id, category_id, bbox and score keys\n",
    "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    predictions = prepare_for_coco_detection(predictions)\n",
    "    evaluator.update(predictions)\n",
    "\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the image_id in target to know which image it is\n",
    "test_idx = 546\n",
    "pixel_values, target = val_dataset[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # forward pass to get class logits and bounding boxes\n",
    "  outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "print(\"Outputs:\", outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.id2label = id2label\n",
    "model.model.config.label2id = label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{id2label[label]}: {score:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image based on ID\n",
    "image_id = target['image_id'].item()\n",
    "image = val_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(val_dir, image['file_name']))\n",
    "\n",
    "# postprocess model outputs\n",
    "width, height = image.size\n",
    "postprocessed_outputs = image_processor.post_process_object_detection(outputs,\n",
    "                                                                target_sizes=[(height, width)],\n",
    "                                                                threshold=0.7)\n",
    "results = postprocessed_outputs[0]\n",
    "plot_results(image, results['scores'], results['labels'], results['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "model_save_path = \"ckpts/detr-simple-ft-240-epochs\"\n",
    "trainer = Trainer(model=model.model, processing_class=image_processor, data_collator=collate_fn, args=TrainingArguments(output_dir=model_save_path))\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
