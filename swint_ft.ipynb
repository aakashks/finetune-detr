{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinTransformer RCNN training and evaluation code \n",
    "This code is based on https://github.com/xiaohu2015/SwinT_detectron2<br>\n",
    "\n",
    "I splitted video 0 and 1 to training and video 2 to validation.<br>\n",
    "So far, I could only train the model for 5 epochs and only achieved:<br>\n",
    "**bbox/AP = 15.2<br>**\n",
    "**bbox/AP50 = 35.7<br>**\n",
    "**bbox/AP75 = 8.0<br>**\n",
    "on the validation set.\n",
    "\n",
    "However, I'd like to share my work and will continue working on it to further improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "install Detectron2, timm and Detectron2 version implementation of SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:26:40.998136Z",
     "iopub.status.busy": "2025-05-15T10:26:40.997842Z",
     "iopub.status.idle": "2025-05-15T10:26:41.004417Z",
     "shell.execute_reply": "2025-05-15T10:26:41.003595Z",
     "shell.execute_reply.started": "2025-05-15T10:26:40.998097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from detectron2.data import (DatasetCatalog, \n",
    "                             MetadataCatalog, \n",
    "                             build_detection_test_loader\n",
    "                            )\n",
    "from detectron2.data.datasets.coco import convert_to_coco_json\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.engine import DefaultTrainer, default_setup, hooks\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
    "from detectron2.evaluation import COCOEvaluator, DatasetEvaluators, inference_on_dataset\n",
    "\n",
    "from swin.swint import add_swint_config\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define a dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from PIL import Image # For getting image dimensions\n",
    "\n",
    "# Your categories definition\n",
    "categories_full = [\n",
    "    {\"id\": 0, \"name\": \"biker\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 1, \"name\": \"car\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 2, \"name\": \"pedestrian\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 3, \"name\": \"trafficlight\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 4, \"name\": \"trafficlight-Green\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 5, \"name\": \"trafficlight-GreenLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 6, \"name\": \"trafficlight-Red\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 7, \"name\": \"trafficlight-RedLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 8, \"name\": \"trafficlight-Yellow\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 9, \"name\": \"trafficlight-YellowLeft\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 10, \"name\": \"truck\", \"supercategory\": \"objects\"},\n",
    "    {\"id\": 11, \"name\": \"Arret\", \"supercategory\": \"objects\"}\n",
    "]\n",
    "# Extract just the names for Detectron2 metadata\n",
    "THING_CLASSES = [cat['name'] for cat in categories_full]\n",
    "\n",
    "# Create a mapping from COCO category_id to a 0-indexed Detectron2 category_id if necessary\n",
    "# If your COCO category_ids are already 0-indexed and contiguous, this might be simpler.\n",
    "# For this example, we'll assume your annotations.json uses the 'id' from categories_full.\n",
    "COCO_CATEGORY_ID_TO_DETECTRON2_ID = {cat['id']: i for i, cat in enumerate(categories_full)}\n",
    "# If your annotations.json already uses 0, 1, 2... for category_id that match the order in THING_CLASSES,\n",
    "# then COCO_CATEGORY_ID_TO_DETECTRON2_ID = {i: i for i in range(len(THING_CLASSES))}\n",
    "\n",
    "\n",
    "def get_custom_coco_dicts(img_dir, annotation_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_dir (str): path to the image directory.\n",
    "        annotation_file_path (str): path to the COCO format annotation json file.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: a list of dicts in Detectron2 dataset format.\n",
    "    \"\"\"\n",
    "    with open(annotation_file_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    # Create a mapping from image_id to image info\n",
    "    images_info = {img['id']: img for img in coco_data['images']}\n",
    "    \n",
    "    # Create a mapping from image_id to annotations\n",
    "    annotations_by_image_id = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in annotations_by_image_id:\n",
    "            annotations_by_image_id[img_id] = []\n",
    "        annotations_by_image_id[img_id].append(ann)\n",
    "\n",
    "    for image_id, img_info in images_info.items():\n",
    "        record = {}\n",
    "        \n",
    "        # Prefer file_name from COCO json if it includes path, otherwise construct it\n",
    "        if 'file_name' in img_info:\n",
    "            # Check if file_name is just the name or includes subdirs\n",
    "            # Assuming your img_dir is the root and file_name might be like \"subdir/image.jpg\"\n",
    "            # or just \"image.jpg\"\n",
    "            filename = os.path.join(img_dir, img_info['file_name'])\n",
    "            # If your 'img_dir' in torchvision.datasets.CocoDetection was the parent of 'images'\n",
    "            # and annotations.json was in 'img_dir', then img_dir here should point to the 'images' subfolder.\n",
    "            # Example: if original img_folder was 'my_dataset/' (containing 'images/' and 'annotations.json')\n",
    "            # then img_dir for this function should be 'my_dataset/images/'\n",
    "        else:\n",
    "            # This case should ideally not happen if coco_data['images'] is complete\n",
    "            print(f\"Warning: 'file_name' not found for image_id {image_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # If image dimensions are not in coco_data['images'], load the image to get them\n",
    "        # It's better if they are pre-computed in the JSON.\n",
    "        height = img_info.get('height')\n",
    "        width = img_info.get('width')\n",
    "        if height is None or width is None:\n",
    "            try:\n",
    "                with Image.open(filename) as pil_img:\n",
    "                    width, height = pil_img.size\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Image file not found at {filename} for image_id {image_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = image_id # Use the original COCO image_id\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        objs = []\n",
    "        if image_id in annotations_by_image_id:\n",
    "            for ann in annotations_by_image_id[image_id]:\n",
    "                # COCO bbox is [x, y, width, height]\n",
    "                # Detectron2 expects BoxMode.XYWH_ABS for this format\n",
    "                coco_bbox = ann['bbox']\n",
    "                \n",
    "                # Get the original category_id from your COCO annotation\n",
    "                original_category_id = ann['category_id']\n",
    "                \n",
    "                # Map it to your 0-indexed Detectron2 category_id\n",
    "                # If your category_ids in the JSON are NOT the same as in `categories_full` `id` field,\n",
    "                # you will need to adjust this mapping.\n",
    "                # This assumes ann['category_id'] corresponds to the 'id' field in categories_full.\n",
    "                try:\n",
    "                    d2_category_id = COCO_CATEGORY_ID_TO_DETECTRON2_ID[original_category_id]\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Unknown category_id {original_category_id} in annotations for image {image_id}. Skipping annotation.\")\n",
    "                    continue\n",
    "\n",
    "                obj = {\n",
    "                    \"bbox\": coco_bbox,\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": d2_category_id,\n",
    "                    \"iscrowd\": ann.get(\"iscrowd\", 0) # Default to 0 if not present\n",
    "                }\n",
    "                objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:26:41.441338Z",
     "iopub.status.busy": "2025-05-15T10:26:41.441130Z",
     "iopub.status.idle": "2025-05-15T10:26:41.447616Z",
     "shell.execute_reply": "2025-05-15T10:26:41.446866Z",
     "shell.execute_reply.started": "2025-05-15T10:26:41.441312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "YOUR_TRAIN_IMG_FOLDER = \"your_actual_train_image_folder_path\" # e.g., /kaggle/input/your_dataset/train/images\n",
    "YOUR_TRAIN_ANN_FILE = \"your_actual_train_annotations_json_path\" # e.g., /kaggle/input/your_dataset/train/annotations.json\n",
    "YOUR_VAL_IMG_FOLDER = \"your_actual_val_image_folder_path\" # e.g., /kaggle/input/your_dataset/val/images\n",
    "YOUR_VAL_ANN_FILE = \"your_actual_val_annotations_json_path\" # e.g., /kaggle/input/your_dataset/val/annotations.json\n",
    "\n",
    "\n",
    "# Registering the datasets\n",
    "for d_type, img_folder, ann_file in [(\"train\", YOUR_TRAIN_IMG_FOLDER, YOUR_TRAIN_ANN_FILE),\n",
    "                                     (\"val\", YOUR_VAL_IMG_FOLDER, YOUR_VAL_ANN_FILE)]:\n",
    "    dataset_name = f\"my_custom_coco_{d_type}\"\n",
    "    # Ensure dataset is not already registered, or pop it first if you're re-running\n",
    "    if dataset_name in DatasetCatalog.list():\n",
    "        DatasetCatalog.pop(dataset_name)\n",
    "    \n",
    "    # The lambda function captures the current values of img_folder and ann_file\n",
    "    DatasetCatalog.register(dataset_name, lambda f=img_folder, a=ann_file: get_custom_coco_dicts(f, a))\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=THING_CLASSES, # Your list of category names\n",
    "                                           evaluator_type=\"coco\", # For COCOEvaluator\n",
    "                                           json_file=ann_file, # Path to the original COCO JSON (for COCOEvaluator)\n",
    "                                           image_root=img_folder # Path to image folder (for COCOEvaluator)\n",
    "                                          )\n",
    "    print(f\"Registered dataset: {dataset_name}\")\n",
    "    print(f\"Associated metadata: {MetadataCatalog.get(dataset_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Check the Dataset\n",
    "visualize the dataset for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:28:00.412850Z",
     "iopub.status.busy": "2025-05-15T10:28:00.412660Z",
     "iopub.status.idle": "2025-05-15T10:30:04.910183Z",
     "shell.execute_reply": "2025-05-15T10:30:04.909563Z",
     "shell.execute_reply.started": "2025-05-15T10:28:00.412828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "my_ds = DatasetCatalog.get(\"my_custom_train\")\n",
    "metadata = MetadataCatalog.get('my_custom_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:30:40.793577Z",
     "iopub.status.busy": "2025-05-15T10:30:40.792862Z",
     "iopub.status.idle": "2025-05-15T10:30:41.059835Z",
     "shell.execute_reply": "2025-05-15T10:30:41.059171Z",
     "shell.execute_reply.started": "2025-05-15T10:30:40.793530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for data in gbl_ds:\n",
    "    if len(data['annotations']):\n",
    "        break\n",
    "im = cv2.cvtColor(cv2.imread(data['file_name']), cv2.COLOR_BGR2RGB)\n",
    "v = Visualizer(im, \n",
    "               metadata=MetadataCatalog.get('my_custom_train'),\n",
    "               scale=0.5)\n",
    "out = v.draw_dataset_dict(data)\n",
    "im = Image.fromarray(out.get_image())\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define a custom Trainer to evaluate on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:30:45.696309Z",
     "iopub.status.busy": "2025-05-15T10:30:45.695697Z",
     "iopub.status.idle": "2025-05-15T10:30:45.702761Z",
     "shell.execute_reply": "2025-05-15T10:30:45.702069Z",
     "shell.execute_reply.started": "2025-05-15T10:30:45.696268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name=dataset_name,\n",
    "                             tasks=[\"bbox\"],\n",
    "                             distributed=True,\n",
    "                             output_dir=output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_tta_model(cls, cfg, model):\n",
    "        return GeneralizedRCNNWithTTA(cfg, model)\n",
    "    \n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        # In the end of training, run an evaluation with TTA\n",
    "        # Only support some R-CNN models.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = self.build_tta_model(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Set a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:30:48.891680Z",
     "iopub.status.busy": "2025-05-15T10:30:48.890944Z",
     "iopub.status.idle": "2025-05-15T10:31:12.961905Z",
     "shell.execute_reply": "2025-05-15T10:31:12.961334Z",
     "shell.execute_reply.started": "2025-05-15T10:30:48.891642Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_STEPS = 4242 # only 4242 images with annotation\n",
    "cfg = get_cfg()\n",
    "add_swint_config(cfg)\n",
    "cfg.merge_from_file('swin/configs/SwinT/faster_rcnn_swint_T_FPN_3x_.yaml')\n",
    "cfg.DATASETS.TRAIN = (\"my_custom_train\",)\n",
    "cfg.DATASETS.TEST = (\"my_val_dataset\",)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.WEIGHTS = None\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.MAX_ITER = TRAIN_STEPS * 10\n",
    "cfg.SOLVER.STEPS = []\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = TRAIN_STEPS\n",
    "cfg.TEST.EVAL_PERIOD = TRAIN_STEPS \n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train\n",
    "\n",
    "I could only run for 5 epochs due to runtime quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:31:17.239952Z",
     "iopub.status.busy": "2025-05-15T10:31:17.239688Z",
     "iopub.status.idle": "2025-05-15T10:31:17.244991Z",
     "shell.execute_reply": "2025-05-15T10:31:17.244297Z",
     "shell.execute_reply.started": "2025-05-15T10:31:17.239921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load pretrained weights\n",
    "other_weights = torch.load('faster_rcnn_swint_T.pth')['model']\n",
    "self_weight = trainer.model.state_dict()\n",
    "for name, param in self_weight.items():\n",
    "    if name in other_weights:\n",
    "        if other_weights[name].shape == param.shape:\n",
    "            self_weight[name] = other_weights[name]\n",
    "        else:\n",
    "            print(f\"size mismatch at {name}\")\n",
    "    else:\n",
    "        print(f\"layer {name} does not exist\")\n",
    "trainer.model.load_state_dict(self_weight)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate\n",
    "evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T10:54:47.014706Z",
     "iopub.status.busy": "2025-05-15T10:54:47.013975Z",
     "iopub.status.idle": "2025-05-15T10:54:49.133565Z",
     "shell.execute_reply": "2025-05-15T10:54:49.132438Z",
     "shell.execute_reply.started": "2025-05-15T10:54:47.014667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.model.load_state_dict(torch.load('/kaggle/input/swintransformer-with-detectron2/model_0021209.pth')['model'])\n",
    "trainer.test(cfg, trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Visualize a few prediction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T11:35:25.514817Z",
     "iopub.status.busy": "2021-12-22T11:35:25.514546Z",
     "iopub.status.idle": "2021-12-22T11:35:25.525685Z",
     "shell.execute_reply": "2021-12-22T11:35:25.524939Z",
     "shell.execute_reply.started": "2021-12-22T11:35:25.514787Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_ds = DatasetCatalog.get(\"my_val_dataset\")\n",
    "trainer.model.eval()\n",
    "metadata = MetadataCatalog.get('my_custom_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T11:46:40.768458Z",
     "iopub.status.busy": "2021-12-22T11:46:40.767662Z",
     "iopub.status.idle": "2021-12-22T11:46:43.058513Z",
     "shell.execute_reply": "2021-12-22T11:46:43.057864Z",
     "shell.execute_reply.started": "2021-12-22T11:46:40.768417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    idx = np.random.randint(0, len(val_ds))\n",
    "    data = val_ds[idx]\n",
    "    print(data['file_name'])\n",
    "    im = cv2.imread(data['file_name'])\n",
    "    im_tensor = torch.from_numpy(im).permute(2,0,1)  # h, w, c -> c, h, w\n",
    "    h, w, _ = im.shape\n",
    "    with torch.no_grad():\n",
    "        pred = trainer.model([{\"image\": im_tensor.cuda(), \"width\": w, \"height\": h}])\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=metadata, \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(pred[0][\"instances\"].to(\"cpu\"))\n",
    "    plt.figure()\n",
    "    plt.imshow(out.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7426230,
     "sourceId": 11822309,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 83162918,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
